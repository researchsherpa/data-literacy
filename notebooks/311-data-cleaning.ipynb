{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b3b5d622-0a2a-4cc7-b648-9c958046c04a",
   "metadata": {
    "tags": []
   },
   "source": [
    "# 311 Data Cleaning\n",
    "\n",
    "This notebook will analyze the the [csv file from empowerla.org](https://data.lacity.org/City-Infrastructure-Service-Requests/MyLA311-Service-Request-Data-2021/97z7-y5bt) pulled on Dec 02 2021.  The analysis, and any changes, should align with [the data transforms](https://github.com/hackforla/311-data/blob/dev/docs/data_loading.md) for the H4LA 311-data project.\n",
    "\n",
    "**Outputs** of the analysis:\n",
    "  * \"Clean\" (my definition of clean!) csv file\n",
    "  * A geodataframe, using the cleaned csv, saved as a shape file and geojson\n",
    "\n",
    "**Steps** in the process are:\n",
    "\n",
    "1.  Read and analyze structure and content\n",
    "2.  Explore data values to determine which fields have valuable information\n",
    "3.  Rename columns on the initial dataframe\n",
    "4.  dtype conversions \n",
    "5.  Create new dataframe with selected columns\n",
    "6.  Create a geodataframe for spatial processing\n",
    "7.  Save both the clean subset of the csv file as csv and the geodataframe as shape file"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0d123d2-6bc8-459f-ae19-da86a6fa7bc6",
   "metadata": {},
   "source": [
    "# 1 - Read and analyze\n",
    "\n",
    "This is a generic look at the csv file.  Pretty standard fare.  Basics include:\n",
    "\n",
    "  * shape\n",
    "  * info\n",
    "  * iloc for one, example instance\n",
    "  * hack to summarize the null value counts\n",
    "  \n",
    "I doubt there is much need for explanation on this step.  This is the first step in deciding which columns provide **good features** for later analysis.\n",
    "\n",
    "**Note** - I'm modifying things a bit for first release.  I'm using pd.read_csv with the actual url for 311 download.  Makes for one less large file.  You can download and use the second pd.read_csv if you prefer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab049248-2e85-46a2-8821-74ec9b6cca67",
   "metadata": {},
   "outputs": [],
   "source": [
    "%run start.py"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ea745ca4-fcbb-44b6-846e-edafb3162ef6",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "myla311_df = pd.read_csv('https://data.lacity.org/api/views/97z7-y5bt/rows.csv', low_memory=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "293c23f6-8da6-4fef-acd6-872850e1709d",
   "metadata": {
    "tags": []
   },
   "source": [
    "Uncomment and use this if you downloaded the file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba168b23-f62e-4739-8000-77194d1d31a6",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "#myla311_df = pd.read_csv('../data/311/MyLA311_Service_Request_Data_2021.csv', \n",
    "#                         low_memory=False)\n",
    "#                        #parse_dates=['CreatedDate', 'UpdatedDate', 'ServiceDate', 'ClosedDate'])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa588b0d-8fda-454d-a7e4-29a50c9b143a",
   "metadata": {},
   "source": [
    "Basic descriptions of the dataframe (df) look at the **shape** and **info** for counts and dtypes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "796edf0d-2ece-46a2-9596-a18e7a00d513",
   "metadata": {},
   "outputs": [],
   "source": [
    "myla311_df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bc51fe6c-eb47-4846-9998-f5745eded358",
   "metadata": {},
   "outputs": [],
   "source": [
    "myla311_df.RequestType.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cca3aa46-bd00-44f2-ad53-d4341d99e8e0",
   "metadata": {},
   "outputs": [],
   "source": [
    "myla311_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0273ffd4-2753-4eeb-95fd-84d9d46ea575",
   "metadata": {
    "tags": []
   },
   "source": [
    "Not sure why, but I always look at the **27**th value first?\n",
    "\n",
    "Here are the values for that row."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a82fe2bd-2bcc-450f-978c-69252b2a125f",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "myla311_df.iloc[27]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c9147c3-6d31-48ff-93f8-05967e6171bb",
   "metadata": {},
   "source": [
    "I use this simple hack to see percentage of null counts in each column of the df.  High numbers, i.e. **MobileOS** at **73%** probably don't provide much value (IMHO).  \n",
    "\n",
    "It's also important to see the ones that are 0.00, i.e. they all have a value."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f318a92c-a44b-48ce-9110-e7430cedaa28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "(myla311_df.isnull().mean() * 100).round(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b9d08b81-a67c-4573-b4d4-8fe7ce41f5a4",
   "metadata": {
    "tags": []
   },
   "source": [
    "At this stage we have some basic views of the data.  We can combine these to determine which columns (variables) could bring value to further analysis.\n",
    "\n",
    "Next we'll look at how we might combine this info.  Remember, our goal is to identify the variables that will provide value for upstream analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ba512043-4b94-4b3a-a85b-6d51e0d94c1c",
   "metadata": {},
   "source": [
    "# 2 - Explore dataframe values\n",
    "\n",
    "OK.  This section is a bit of a **digression!**  I'm using some basic jupy widgetry to combine the outputs from the first section.\n",
    "\n",
    "30K foot view of this section:\n",
    "\n",
    "   1.  Create Output widgets for info and null value displays\n",
    "   2.  Build interactive function to display value_counts for selected column name\n",
    "   3.  Combine via HBox widget to show them together"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e1ca25a-1ec3-4dba-9b4c-aff7f8f9d417",
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df = Output(layout={'border': '1px solid black',\n",
    "                            'width': '50%'})\n",
    "\n",
    "null_info = Output(layout={'border': '1px solid black',\n",
    "                            'width': '30%'})\n",
    "\n",
    "with info_df:\n",
    "    display(HTML('<center><b>myla311_df info()</b></center>'))\n",
    "    display(myla311_df.info())\n",
    "    \n",
    "with null_info:\n",
    "    display(HTML('<center><b>normalized info()</b></center>'))\n",
    "    display((myla311_df.isnull().mean() * 100).round(2))\n",
    "\n",
    "from ipywidgets import interact, interactive\n",
    "\n",
    "def examine_value_counts(col):\n",
    "    display(myla311_df[col].value_counts())\n",
    "    \n",
    "    \n",
    "w = interactive(examine_value_counts, col=list(myla311_df.columns))\n",
    "w;"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5221f646-af88-4910-9537-7ed44e7e56cc",
   "metadata": {},
   "source": [
    "At this point use the info and null value displays to identify columns to explore.  Couple of examples to explore:\n",
    "\n",
    "  1.  Note that RequestType is 0.00 null values (i.e. all rows have a value).  Check out the value_counts by selecting RequestType.\n",
    "  2.  Check out CD/CDMember and NC/NCName.  Need to do something about the floats?\n",
    "  3.  ...  You can choose some to explore!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "db069102-bb30-42e8-b23a-d61fcd61fd14",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "HBox([info_df, null_info, w])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "714db699-23de-4ec1-b5cd-e073613df7ff",
   "metadata": {},
   "source": [
    "# 3 - Rename columns\n",
    "\n",
    "mapping_311 is the dictionary I created in my editor.  I edited the dictionary generated from this comprehension:\n",
    "\n",
    "```python\n",
    "{v[0], v[1] for v in zip(myla311_df.columns.to_list(), myla311_df.columns.to_list()}\n",
    "```       \n",
    "\n",
    "I tried to make the names more pythonic.\n",
    "\n",
    "Here's my mapping dict."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e310f4ad-56a2-4b88-8e27-ad26fc3a0933",
   "metadata": {},
   "outputs": [],
   "source": [
    "mapping_311 = {'SRNumber': 'SRNumber',\n",
    "               'CreatedDate': 'created_dt',\n",
    "               'UpdatedDate': 'updated_dt',\n",
    "               'ActionTaken': 'ActionTaken',\n",
    "               'Owner': 'owner',\n",
    "               'RequestType': 'request_type',\n",
    "               'Status': 'Status',\n",
    "               'RequestSource': 'RequestSource',\n",
    "               'MobileOS': 'MobileOS',\n",
    "               'Anonymous': 'Anonymous',\n",
    "               'AssignTo': 'AssignTo',\n",
    "               'ServiceDate': 'service_dt',\n",
    "               'ClosedDate': 'closed_dt',\n",
    "               'AddressVerified': 'AddressVerified',\n",
    "               'ApproximateAddress': 'ApproximateAddress',\n",
    "               'Address': 'address',\n",
    "               'HouseNumber': 'HouseNumber',\n",
    "               'Direction': 'Direction',\n",
    "               'StreetName': 'street',\n",
    "               'Suffix': 'Suffix',\n",
    "               'ZipCode': 'zip_code',\n",
    "               'Latitude': 'latitude',\n",
    "               'Longitude': 'longitude',\n",
    "               'Location': 'location',\n",
    "               'TBMPage': 'TBMPage',\n",
    "               'TBMColumn': 'TBMColumn',\n",
    "               'TBMRow': 'TBMRow',\n",
    "               'APC': 'APC',\n",
    "               'CD': 'cd',\n",
    "               'CDMember': 'cd_member',\n",
    "               'NC': 'nc',\n",
    "               'NCName': 'nc_name',\n",
    "               'PolicePrecinct': 'precinct'\n",
    "}"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b350635-8515-4589-a6ed-ce113ad165b4",
   "metadata": {},
   "source": [
    "I'm going to hold off on renaming for a bit.  \n",
    "\n",
    "**Note to self** - So in the next section I'm adding new columns ... how does that relate to this dictionary?  Maybe I should do this process after next section?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d16ef14d-3fe3-48f9-8543-beec1b9f713d",
   "metadata": {},
   "source": [
    "# 4 - dtype conversions and new columns\n",
    "\n",
    "Conversions (for starters):\n",
    "\n",
    "> * dates - date information is object dtype (str) so convert to python dtime\n",
    "> * ids - floats, convert to int\n",
    "\n",
    "New colums are added for duration of the request.\n",
    "\n",
    "> * days_to_service\n",
    "> * days_to_close\n",
    "> * days_to_update\n",
    "\n",
    "For the duration code I'm borrowing [work from the 311 data science team](https://colab.research.google.com/drive/1LvuuPDWPGC6g3WOLHOk806ocQbYAJ5qj#scrollTo=g-H5Fq91BSYW).  Chelsey shared this collab link (thanks Chelsey).  \n",
    "Honestly, I'm not sure what to do with these yet?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d4f94076-c0ed-4b15-ac66-f1f4b0a539ce",
   "metadata": {},
   "outputs": [],
   "source": [
    "myla311_df['CreatedDate'] = pd.to_datetime(myla311_df['CreatedDate'])\n",
    "myla311_df['ServiceDate'] = pd.to_datetime(myla311_df['ServiceDate'])\n",
    "myla311_df['ClosedDate'] = pd.to_datetime(myla311_df['ClosedDate'])\n",
    "myla311_df['UpdatedDate'] = pd.to_datetime(myla311_df['UpdatedDate'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfcc0769-5636-4307-bcd1-76ba18a89acd",
   "metadata": {},
   "outputs": [],
   "source": [
    "myla311_df['days_to_service'] = (myla311_df.ServiceDate - myla311_df.CreatedDate).astype('timedelta64[D]')\n",
    "myla311_df['days_to_close'] = (myla311_df.ClosedDate - myla311_df.CreatedDate).astype('timedelta64[D]')\n",
    "myla311_df['days_to_update'] = (myla311_df.UpdatedDate - myla311_df.CreatedDate).astype('timedelta64[D]')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78dc57d9-cd9a-4eb2-a48d-a99051777bb4",
   "metadata": {},
   "outputs": [],
   "source": [
    "myla311_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "569a005b-f8eb-4c2b-8b2e-b09878519666",
   "metadata": {},
   "source": [
    "So geopandas driver for esri shape doesn't support dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b912b147-49fc-4bd1-9c25-3da07169d8d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "#myla311_df['CreatedDate'] = myla311_df['CreatedDate'].astype(str)\n",
    "\n",
    "#myla311_df['ServiceDate'] = myla311_df['ServiceDate'].astype(str)\n",
    "#myla311_df['ClosedDate'] = myla311_df['ClosedDate'].astype(str)\n",
    "#myla311_df['UpdatedDate'] = myla311_df['UpdatedDate'].astype(str)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0516459-3c5b-45f6-ae71-bc9679a1bf70",
   "metadata": {},
   "source": [
    "Since there are NaN's in NC and CD I'm using this idea https://stackoverflow.com/questions/21287624/convert-pandas-column-containing-nans-to-dtype-int"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1436554-2d5c-40ee-afa5-60a562498201",
   "metadata": {},
   "outputs": [],
   "source": [
    "myla311_df['NC'] = myla311_df['NC'].astype('Int64')\n",
    "myla311_df['CD'] = myla311_df['CD'].astype('Int64')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bf2b9227-b63d-408e-afc7-eca31ab66d21",
   "metadata": {},
   "outputs": [],
   "source": [
    "myla311_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ddb01068-2199-4a46-9983-3c0e7852a9d3",
   "metadata": {},
   "source": [
    "# 5 - Create New Dataframe\n",
    "\n",
    "I'm cheating a bit.  I have some idea of the types of analysis I want to do later, so that is driving which columns I want to include for now.  Not sure why I'm keeping location?\n",
    "\n",
    "Steps in this section:\n",
    "\n",
    "1.  create new df from myla311_df\n",
    "2.  use the mapping dictionary to rename the columns\n",
    "3.  Use the list of columns to select for a new df.\n",
    "\n",
    "The result of this step is the new311_df.  myla311 is still intact, albeit with cleaned columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d61fb70-82e5-4872-82e8-600096c8c256",
   "metadata": {},
   "outputs": [],
   "source": [
    "column_subset = ['SRNumber',\n",
    "                 'created_dt',\n",
    "                 'updated_dt',\n",
    "                 'owner',\n",
    "                 'request_type',\n",
    "                 'service_dt',\n",
    "                 'closed_dt',\n",
    "                 'address',\n",
    "                 'street',\n",
    "                 'zip_code',\n",
    "                 'latitude',\n",
    "                 'longitude',\n",
    "                 'location',\n",
    "                 'APC',\n",
    "                 'cd',\n",
    "                 'cd_member',\n",
    "                 'nc',\n",
    "                 'nc_name',\n",
    "                 'precinct',\n",
    "                 'days_to_service',\n",
    "                 'days_to_close',\n",
    "                 'days_to_update'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d88a93bd-7f90-4e99-979e-0b54595f852c",
   "metadata": {},
   "outputs": [],
   "source": [
    "new311_df = myla311_df.rename(columns=mapping_311)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "adc62102-e04f-4cec-868b-897441ab9b77",
   "metadata": {},
   "outputs": [],
   "source": [
    "new311_df = new311_df[column_subset]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c5033252-dbe6-48e2-97b1-0133cb60579d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new311_df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7df9649-7bc1-47cb-99f7-76b1bee11643",
   "metadata": {},
   "source": [
    "So this df looks ok?\n",
    "\n",
    "**Note to self** - really need to abstract the widgets above so we can apply the same viz to this df!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59833beb-bc4c-415b-a229-f292995f51a1",
   "metadata": {},
   "source": [
    "Save the clean csv file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53444603-bbfe-4921-aaa1-ef34775edadd",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "new311_df.to_csv('../data/311/clean311.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21870de7-1f7c-4c52-85a9-0441575e2eba",
   "metadata": {},
   "source": [
    "# 6 - Create the GeoDataframe\n",
    "\n",
    "Use the cleaned csv file to generate a geodataframe then save (shape, geojson, zip).\n",
    "\n",
    "The only _tricks_ here are:\n",
    "\n",
    "1. Make sure the row has valid geo content\n",
    "2. Create the geometry as wkt Point feature (note Point get's us to x, y so need to use longitude, latitiude order)\n",
    "3. Save the geodataframe in multiple formats\n",
    "4. Because geopandas uses ESRI driver and it's limited to 10 character names have to jump through a hoop\n",
    "\n",
    "At the end of this section there are four addtional files written to ../data/311."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "099db480-0eb5-425f-b0df-498e23c8f885",
   "metadata": {},
   "outputs": [],
   "source": [
    "(new311_df.location.isnull().sum()) / len(new311_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "011a1144-1c63-46e5-90d2-54c9e71c43ae",
   "metadata": {},
   "source": [
    "An extremely small number of rows don't have location so just toss 'em."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "490820f9-1741-4d25-8936-3c1e7e6129c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "geocodeable311_df = new311_df[new311_df.location.notnull()]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd254833-3241-4ab0-9198-b3a5eb654d9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "new311_gdf = GeoDataFrame(geocodeable311_df,\n",
    "                          geometry = [Point(xy[0], xy[1]) for xy in zip(geocodeable311_df['longitude'], geocodeable311_df['latitude'])])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee0c0de9-8109-4920-8411-b910a95d5b49",
   "metadata": {},
   "outputs": [],
   "source": [
    "new311_gdf.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc106418-a5c9-4a7a-85e1-225d68f32a98",
   "metadata": {},
   "outputs": [],
   "source": [
    "new311_gdf['created_dt'] = new311_gdf['created_dt'].astype(str)\n",
    "\n",
    "new311_gdf['service_dt'] = new311_gdf['service_dt'].astype(str)\n",
    "new311_gdf['closed_dt'] = new311_gdf['closed_dt'].astype(str)\n",
    "new311_gdf['updated_dt'] = new311_gdf['updated_dt'].astype(str)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eb3febb6-eda1-4990-ab96-989f6190faf2",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "new311_gdf.to_file('../data/311/clean311-geo.shp', index=False)\n",
    "#new311_gdf.to_file('/home/mcmorgan/for-sarah/clean311-geo.shp', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05b2455f-67fc-4670-8088-1c2bdfed1843",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "%%time\n",
    "new311_gdf.to_file('../data/311/clean311.geojson', driver='GeoJSON')\n",
    "#new311_gdf.to_file('/home/mcmorgan/for-sarah/clean311.geojson', driver='GeoJSON')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0941600a-38a3-4bc7-9921-30bf966d4468",
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "new311_gdf.to_file('../data/311/clean311-geo.zip', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4a259d5-403f-4144-ae7f-433f197f90fc",
   "metadata": {},
   "source": [
    "At this point four new data sets have been added to ../data/311\n",
    "\n",
    "1.  clean311.csv - the subset with columns used to generate the geodataframes\n",
    "2.  clean311-geo.shp - There are three other files for the shapefile spec, but this is the one to read back in\n",
    "3.  clean311.geojson - The geojson version of the data frame\n",
    "4.  clean311-geo.zip - A zipped shape folder\n",
    "\n",
    "I've added a function in src/utils.py, read_new311_shape to read the shape/zip files and do the dtype conversions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2cd8041-4da6-4632-8673-5fd8c75a6449",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "!ls -alh --time-style=+%D ../data/311 | grep $(date +%D)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "52ac6768-87cd-459f-aad3-d7d4381a7124",
   "metadata": {},
   "source": [
    "went out to the command line and zipped these files up.  Hoping to fit them into the repo with lfs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7642587d-b48f-474b-b4c6-a2821c6a78ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "!ls -alh --time-style=+%D ../data/311 | grep $(date +%D)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a918fcf-84a5-4da5-bd95-d79c57a9ed86",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
